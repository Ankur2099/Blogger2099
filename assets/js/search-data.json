{
  
    
        "post0": {
            "title": "Deep Learning With Pytorch",
            "content": "&lt;!DOCTYPE html&gt; . 2020-03-15-Deep-Learning-with-Pytorch . Deep Learning with Pytorch 1.0 - Pets Classification Project . Welcome to my Project-cum-Blogpost. In this one, I will be building an image classifier for Pets from scratch, and make it as accurate as possible. . I&#39;ll be trying to explain every line of code as much as possible throughout the project. . This project uses fastai V1 Library which sits on the top of Pytorch 1.0. The fastai library provides many useful functions that enables us to quickly and easily build Artificial neural networks and train our models. . Artificial neural network - Artificial neural networks (ANN) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems &quot;learn&quot; to perform tasks by considering examples, generally without being programmed with task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as &quot;cat&quot; or &quot;no cat&quot; and using the results to identify cats in other images. They do this without any prior knowledge of cats, for example, that they have fur, tails, whiskers and cat-like faces. Instead, they automatically generate identifying characteristics from the examples that they process. . An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it. . I am using Google Colab which comes with a GPU support for free. It uses Nvidia Tesla K80 with 12GB of RAM which I ll be using for most of my projects. . In&nbsp;[0]: %reload_ext autoreload %autoreload 2 %matplotlib inline . Every notebook starts with the following three lines of code. They ensure that any edits to libraries we make are reloaded here automatically, and also that any charts or images displayed or shown in this notebook. . Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. The % sign is called as Magics. Its not a part of python programming. It just renders our images or plots within our Jupyter notebook. . In&nbsp;[0]: from fastai.vision import * from fastai.metrics import error_rate . The fastai.vision is written for computer vision or we can say image applications. . We use metrics for evaluation of machine learning algorithm&#39;s performance. There are a lot of them. We ll see that as I use it during the project. . In&nbsp;[0]: bs = 32 . The Batch Size defines the number of samples that will be propagated through the network. We can use bigger batch size, but it requires more and more GPU RAM. . Looking at the data . We are going to use the Oxford-IIIT Pet Dataset by O. M. Parkhi et al., 2012 which features 12 cat breeds and 25 dogs breeds. Our model will need to learn to differentiate between these 37 distinct categories. According to their paper, the best accuracy they could get in 2012 was 59.21%, using a complex model that was specific to pet detection, with separate &quot;Image&quot;, &quot;Head&quot;, and &quot;Body&quot; models for the pet photos. Let&#39;s see how accurate we can be using Deep Learning! . In&nbsp;[0]: help(untar_data) . Help on function untar_data in module fastai.datasets: untar_data(url:str, fname:Union[pathlib.Path, str]=None, dest:Union[pathlib.Path, str]=None, data=True, force_download=False) -&gt; pathlib.Path Download `url` to `fname` if `dest` doesn&#39;t exist, and un-tgz to folder `dest`. . We are going to use the untar_data function to which we must pass a URL as an argument and which will download and extract the data. . In&nbsp;[0]: path = untar_data(URLs.PETS) path . Downloading https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet . Out[0]: PosixPath(&#39;/root/.fastai/data/oxford-iiit-pet&#39;) . In&nbsp;[0]: path.ls() . Out[0]: [PosixPath(&#39;/root/.fastai/data/oxford-iiit-pet/annotations&#39;), PosixPath(&#39;/root/.fastai/data/oxford-iiit-pet/images&#39;)] . In&nbsp;[0]: path_anno = path/&#39;annotations&#39; path_img = path/&#39;images&#39; . The first thing we do when we approach a problem is to take a look at the data. We always need to understand very well what the problem is and what the data looks like before we can figure out how to solve it. Taking a look at the data means understanding how the data directories are structured, what the labels are and what some sample images look like. . The main difference between the handling of image classification datasets is the way labels are stored. In this particular dataset, labels are stored in the filenames themselves. We will need to extract them to be able to classify the images into the correct categories. Fortunately, the fastai library has a handy function made exactly for this, ImageDataBunch.from_name_re gets the labels from the file names using a Regular Expressions or Regex . In&nbsp;[0]: fnames = get_image_files(path_img) fnames[:5] . Out[0]: [PosixPath(&#39;/root/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_112.jpg&#39;), PosixPath(&#39;/root/.fastai/data/oxford-iiit-pet/images/keeshond_61.jpg&#39;), PosixPath(&#39;/root/.fastai/data/oxford-iiit-pet/images/Siamese_77.jpg&#39;), PosixPath(&#39;/root/.fastai/data/oxford-iiit-pet/images/leonberger_17.jpg&#39;), PosixPath(&#39;/root/.fastai/data/oxford-iiit-pet/images/saint_bernard_193.jpg&#39;)] . In&nbsp;[0]: np.random.seed(2) pat = r&#39;/([^/]+)_ d+.jpg$&#39; . In&nbsp;[0]: data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs) data.normalize(imagenet_stats) . Out[0]: ImageDataBunch; Train: LabelList (5912 items) x: ImageList Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224) y: CategoryList staffordshire_bull_terrier,keeshond,Siamese,havanese,wheaten_terrier Path: /root/.fastai/data/oxford-iiit-pet/images; Valid: LabelList (1478 items) x: ImageList Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224) y: CategoryList boxer,British_Shorthair,english_cocker_spaniel,Russian_Blue,keeshond Path: /root/.fastai/data/oxford-iiit-pet/images; Test: None . The above line tells fastai what kind of dataset we have, and how it is structured. There are various different classes for different kinds of deep learning dataset and problem. Here we&#39;re using ImageDataBunch. The first part of the class name will generally be the type of data you have, such as image, or text. The second part will generally be getting the file names. The third part refers to our pattern of Regex. . We define the Transforms that we need. A Transform contains code that is applied automatically during training; fastai includes many pre-defined Transforms. df_tfms is applied to a batch of items at a time using the GPU, so they&#39;re particularly fast. . Why 224 pixels? This is the standard size for historical reasons (old pretrained models require this size exactly), but we can pass pretty much anything. If we increase the size, we&#39;ll often get a model with better results (since it will be able to focus on more details) but at the price of speed and memory consumption; or vice versa if you decrease the size. . The data.normalize function converts the image into pixel values of color channels (RGB) which ranges from 0 - 255, converting it into matrix form. This is where our GPU is very good at i.e. Matrix Operations. . In&nbsp;[0]: data.show_batch(rows=3, figsize=(7,6)) . In&nbsp;[0]: print(data.classes) len(data.classes), data.c . [&#39;Abyssinian&#39;, &#39;Bengal&#39;, &#39;Birman&#39;, &#39;Bombay&#39;, &#39;British_Shorthair&#39;, &#39;Egyptian_Mau&#39;, &#39;Maine_Coon&#39;, &#39;Persian&#39;, &#39;Ragdoll&#39;, &#39;Russian_Blue&#39;, &#39;Siamese&#39;, &#39;Sphynx&#39;, &#39;american_bulldog&#39;, &#39;american_pit_bull_terrier&#39;, &#39;basset_hound&#39;, &#39;beagle&#39;, &#39;boxer&#39;, &#39;chihuahua&#39;, &#39;english_cocker_spaniel&#39;, &#39;english_setter&#39;, &#39;german_shorthaired&#39;, &#39;great_pyrenees&#39;, &#39;havanese&#39;, &#39;japanese_chin&#39;, &#39;keeshond&#39;, &#39;leonberger&#39;, &#39;miniature_pinscher&#39;, &#39;newfoundland&#39;, &#39;pomeranian&#39;, &#39;pug&#39;, &#39;saint_bernard&#39;, &#39;samoyed&#39;, &#39;scottish_terrier&#39;, &#39;shiba_inu&#39;, &#39;staffordshire_bull_terrier&#39;, &#39;wheaten_terrier&#39;, &#39;yorkshire_terrier&#39;] . Out[0]: (37, 37) . Here, data.classes and data.c are used to print the names of the classes in our dataset. . Training: resnet34 . Now we will start training our model. We will use a convolutional neural network backbone and a fully connected head with a single hidden layer as a classifier. . We are going to use a method called Transfer Learning here. . Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks. We can use it for all kinds of visual recognition tasks. It takes 1/100th of the time and also 1/100th of the data to give us state-of-the-art results which we ll find out later. . Using a pretrained models is the most important method we have to allow us to train more accurate models, more quickly, with less data, and less time and money. . resnet34 as the name suggests has 34 fully connected neural network which has arready been trained on 1.3 million images on the ImageNet Dataset. We are just using the weights that that the model has already learned from it. . What is meant by weights in machine learning? Weights are used to connect the each neurons in one layer to the every neurons in the next layer. Weights near zero mean changing this input will not change the output. . We are building a model which will take images as input and will output the predicted probability for each of the categories (in this case, it will have 37 outputs). . We will train for 4 epochs (4 cycles through all our data). . What are epochs in Machine Learning? Each time the model has sees the whole training data. . In&nbsp;[0]: learn = cnn_learner(data, models.resnet34, metrics=error_rate) . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth . . learn knows three things - 1. What&#39;s my data 2.What&#39;s my architecture 3.What metric is being used. . cnn_learner also has a parameter pretrained, which defaults to True (so it&#39;s used in this case), which sets the weights in your model to values that have already been trained by experts to recognize a thousand different categories across 1.3 million photos (using the famous ImageNet dataset). . We must divide our data into three sets. A training set, a test set and a Validation set. . Training set - The data used for fitting the model which is 80% of our data; it does not include any data from the validation set. . Validation set - 20% of our data which is set of data held out from training, used only for measuring how good the model is called our Validation set which helps us avoid overfitting. . Test set - A test set is a dataset that is independent of the training dataset, but that follows the same probability distribution as the training dataset. If a model fit to the training dataset also fits the test dataset well,minimal overfitting has taken place. A better fitting of the training dataset as opposed to the test dataset usually points to overfitting. . Overfitting - Training a model in such a way that it remembers specific features of the input data, rather than generalizing well to data not seen during training. Neural networks are really good at memorizing data and we got to avoid that. That is why validation set is always necessary. . Architecture - resnet34 is basically the Architecture for our model which is the template of the model that we&#39;re trying to fit; the actual mathematical function that we&#39;re passing the input data and parameters to. . In&nbsp;[0]: learn.model . Out[0]: Sequential( (0): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (4): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (5): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten() (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=1024, out_features=512, bias=True) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=37, bias=True) ) ) . In&nbsp;[0]: learn.fit_one_cycle(4) . epoch train_loss valid_loss error_rate time . 0 | 0.958631 | 0.319353 | 0.099459 | 01:40 | . 1 | 0.571395 | 0.299972 | 0.100812 | 01:39 | . 2 | 0.401943 | 0.206789 | 0.068336 | 01:40 | . 3 | 0.324712 | 0.212774 | 0.069012 | 01:38 | . Our model is around 93% Accurate. . fit_one_cycle is what starts our training process. For more details, please refer the article - one-cycle-policy. This explains all the mathematical details behind this concept brilliantly. . In&nbsp;[0]: learn.save(&#39;stage-1&#39;) . learn.save() saves the learned weights/co-efficients. We ll save our original model so that we can use it later to fine tune things if required. . Results . We will first see which were the categories that the model most confused with one another. We will try to see if what the model predicted was reasonable or not. In this case the mistakes look reasonable. This is an indicator that our classifier is working correctly. . Furthermore, when we plot the confusion matrix, we can see that the distribution is heavily skewed: the model makes the same mistakes over and over again but it rarely confuses other categories. This suggests that it just finds it difficult to distinguish some specific categories between each other; this is normal behaviour. . In&nbsp;[0]: interp = ClassificationInterpretation.from_learner(learn) losses,idxs = interp.top_losses() len(data.valid_ds)==len(losses)==len(idxs) . Out[0]: True . Loss Function - It&#39;s a method of evaluating how well specific algorithm models the given data. . In&nbsp;[0]: interp.plot_top_losses(9, figsize=(15,11)) . plot_top_losses function - Lets see the documentation for fastai library. It explains each and every argument/results very well, which are also self-explanatory and easy to read. We ll use the doc() function for it. . In&nbsp;[0]: doc(interp.plot_top_losses) . _cl_int_plot_top_losses[source][test] . _cl_int_plot_top_losses(k, largest=True, figsize=(12, 12), heatmap:bool=False, heatmap_thresh:int=16, alpha:float=0.6, cmap:str=&#39;magma&#39;, show_text:bool=True, return_fig:bool=None) → Optional[Figure] . &times;No tests found for _cl_int_plot_top_losses. To contribute a test please refer to this guide and this discussion. . Show images in top_losses along with their prediction, actual, loss, and probability of actual class. . Show in docs . In&nbsp;[0]: interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . Whats a Confusion Matrix? . In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one. Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another). . It is a special kind of contingency table, with two dimensions (&quot;actual&quot; and &quot;predicted&quot;), and identical sets of &quot;classes&quot; in both dimensions (each combination of dimension and class is a variable in the contingency table). . Intuitively, we can say that the dark colored boxes represent the number of times the prediction by our model being actually correct. . In&nbsp;[0]: interp.most_confused(min_val=2) . Out[0]: [(&#39;Bengal&#39;, &#39;Egyptian_Mau&#39;, 6), (&#39;Ragdoll&#39;, &#39;Birman&#39;, 5), (&#39;Birman&#39;, &#39;Ragdoll&#39;, 4), (&#39;Maine_Coon&#39;, &#39;Ragdoll&#39;, 4), (&#39;basset_hound&#39;, &#39;beagle&#39;, 4), (&#39;yorkshire_terrier&#39;, &#39;havanese&#39;, 4), (&#39;Bengal&#39;, &#39;Maine_Coon&#39;, 3), (&#39;Birman&#39;, &#39;Siamese&#39;, 3), (&#39;Russian_Blue&#39;, &#39;British_Shorthair&#39;, 3), (&#39;Siamese&#39;, &#39;Birman&#39;, 3), (&#39;american_pit_bull_terrier&#39;, &#39;staffordshire_bull_terrier&#39;, 3), (&#39;miniature_pinscher&#39;, &#39;staffordshire_bull_terrier&#39;, 3), (&#39;Bengal&#39;, &#39;Abyssinian&#39;, 2), (&#39;British_Shorthair&#39;, &#39;Bombay&#39;, 2), (&#39;British_Shorthair&#39;, &#39;Russian_Blue&#39;, 2), (&#39;Egyptian_Mau&#39;, &#39;Abyssinian&#39;, 2), (&#39;Ragdoll&#39;, &#39;Persian&#39;, 2), (&#39;american_bulldog&#39;, &#39;american_pit_bull_terrier&#39;, 2), (&#39;american_bulldog&#39;, &#39;staffordshire_bull_terrier&#39;, 2), (&#39;beagle&#39;, &#39;basset_hound&#39;, 2), (&#39;staffordshire_bull_terrier&#39;, &#39;american_pit_bull_terrier&#39;, 2)] . most_confused - fastai Library provides this great function for a better understanding of the results. The first argument represents &quot;The Actual Class or Label&quot;, the second one represents &quot;What our model Predicted&quot; and the third one says &quot;How many times it was predicted wrong by our model&quot; . Unfreezing, fine-tuning and Learning Rates . Since our model is working as we expect it to, we will unfreeze our model and train some more and lets see if we can make our model better. . In&nbsp;[0]: learn.unfreeze() . Unfreeze - It just says &quot;Please train the whole model&quot;. It trains all the layers at same speed. . In&nbsp;[0]: learn.fit_one_cycle(1) . epoch train_loss valid_loss error_rate time . 0 | 0.837055 | 0.332106 | 0.106225 | 01:47 | . As we can see that our error got worse. . In&nbsp;[0]: learn.load(&#39;stage-1&#39;); . Lets load our previously saved model. . In&nbsp;[0]: learn.lr_find() . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 0.00% [0/1 00:00&lt;00:00] epoch train_loss valid_loss error_rate time . &lt;progress value=&#39;59&#39; class=&#39;&#39; max=&#39;184&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 32.07% [59/184 00:29&lt;01:03 0.8478] &lt;/div&gt; &lt;/div&gt; LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Learning Rate - It basically means &quot;How quickly am I updating the weights in our model&quot;. . lr_find function helps us to find the best learning rate for our model. . In&nbsp;[0]: learn.recorder.plot() . We can see our losses are minimum between the learning rates 1e-06 and 1e-04. Rest of the layers&#39; learning rates are distributed between the two values. . In&nbsp;[0]: learn.unfreeze() learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-4)) . epoch train_loss valid_loss error_rate time . 0 | 0.285862 | 0.196806 | 0.066982 | 01:45 | . 1 | 0.240074 | 0.186457 | 0.058187 | 01:44 | . We can see that our Model is around 94.2% Accurate and hence, we conclude our Project. . &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; .",
            "url": "https://ankur2099.github.io/Blogger2099/2020/03/15/Deep-Learning-with-Pytorch.html",
            "relUrl": "/2020/03/15/Deep-Learning-with-Pytorch.html",
            "date": " • Mar 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc: true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ankur2099.github.io/Blogger2099/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ankur2099.github.io/Blogger2099/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ankur2099.github.io/Blogger2099/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}