{
  
    
        "post0": {
            "title": "Deep learning with pytorch_1.0 Pets classification project",
            "content": "&lt;!DOCTYPE html&gt; . Deep_Learning_with_fastai_and_Pytorch_1_0_Pets_Classification_Project . Deep Learning with fastai and Pytorch 1.0 - Pets Classification Project . Welcome to my Project-cum-Blogpost. In this one, I will be building an image classifier for Pets from scratch, and make it as accurate as possible. . I&#39;ll be trying to explain every line of code as much as possible throughout the project. . This project uses fastai V1 Library which sits on the top of Pytorch 1.0. The fastai library provides many useful functions that enables us to quickly and easily build Artificial neural networks and train our models. . Artificial neural network - Artificial neural networks (ANN) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems &quot;learn&quot; to perform tasks by considering examples, generally without being programmed with task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as &quot;cat&quot; or &quot;no cat&quot; and using the results to identify cats in other images. They do this without any prior knowledge of cats, for example, that they have fur, tails, whiskers and cat-like faces. Instead, they automatically generate identifying characteristics from the examples that they process. . An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it. . I am using Google Colab which comes with a GPU support for free. It uses Nvidia Tesla K80 with 12GB of RAM which I ll be using for most of my projects. . In&nbsp;[0]: %reload_ext autoreload %autoreload 2 %matplotlib inline . Every notebook starts with the following three lines of code. They ensure that any edits to libraries we make are reloaded here automatically, and also that any charts or images displayed or shown in this notebook. . Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. The % sign is called as Magics. Its not a part of python programming. It just renders our images or plots within our Jupyter notebook. . In&nbsp;[0]: from fastai.vision import * from fastai.metrics import error_rate . The fastai.vision is written for computer vision or we can say image applications. . We use metrics for evaluation of machine learning algorithm&#39;s performance. There are a lot of them. We ll see that as I use it during the project. . In&nbsp;[0]: bs = 32 . The Batch Size defines the number of samples that will be propagated through the network. We can use bigger batch size, but it requires more and more GPU RAM. . Looking at the data&lt;/b&gt;&lt;/center&gt;&lt;/h3&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; We are going to use the Oxford-IIIT Pet Dataset by O. M. Parkhi et al., 2012 which features 12 cat breeds and 25 dogs breeds. Our model will need to learn to differentiate between these 37 distinct categories. According to their paper, the best accuracy they could get in 2012 was 59.21%, using a complex model that was specific to pet detection, with separate &quot;Image&quot;, &quot;Head&quot;, and &quot;Body&quot; models for the pet photos. Let&#39;s see how accurate we can be using Deep Learning! . In&nbsp;[4]: help(untar_data) . Help on function untar_data in module fastai.datasets: untar_data(url:str, fname:Union[pathlib.Path, str]=None, dest:Union[pathlib.Path, str]=None, data=True, force_download=False) -&gt; pathlib.Path Download `url` to `fname` if `dest` doesn&#39;t exist, and un-tgz to folder `dest`. . We are going to use the untar_data function to which we must pass a URL as an argument and which will download and extract the data. . In&nbsp;[5]: path = untar_data(URLs.PETS) path . Downloading https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet . Out[5]: PosixPath(&#39;/root/.fastai/data/oxford-iiit-pet&#39;) . In&nbsp;[6]: path.ls() . Out[6]: [PosixPath(&#39;/root/.fastai/data/oxford-iiit-pet/annotations&#39;), PosixPath(&#39;/root/.fastai/data/oxford-iiit-pet/images&#39;)] . In&nbsp;[0]: path_anno = path/&#39;annotations&#39; path_img = path/&#39;images&#39; . The first thing we do when we approach a problem is to take a look at the data. We always need to understand very well what the problem is and what the data looks like before we can figure out how to solve it. Taking a look at the data means understanding how the data directories are structured, what the labels are and what some sample images look like. . The main difference between the handling of image classification datasets is the way labels are stored. In this particular dataset, labels are stored in the filenames themselves. We will need to extract them to be able to classify the images into the correct categories. Fortunately, the fastai library has a handy function made exactly for this, ImageDataBunch.from_name_re gets the labels from the file names using a Regular Expressions or Regex . In&nbsp;[8]: fnames = get_image_files(path_img) fnames[:5] . Out[8]: [PosixPath(&#39;/root/.fastai/data/oxford-iiit-pet/images/pomeranian_53.jpg&#39;), PosixPath(&#39;/root/.fastai/data/oxford-iiit-pet/images/saint_bernard_137.jpg&#39;), PosixPath(&#39;/root/.fastai/data/oxford-iiit-pet/images/scottish_terrier_191.jpg&#39;), PosixPath(&#39;/root/.fastai/data/oxford-iiit-pet/images/english_cocker_spaniel_130.jpg&#39;), PosixPath(&#39;/root/.fastai/data/oxford-iiit-pet/images/german_shorthaired_152.jpg&#39;)] . In&nbsp;[0]: np.random.seed(2) pat = r&#39;/([^/]+)_ d+.jpg$&#39; . In&nbsp;[10]: data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs) data.normalize(imagenet_stats) . Out[10]: ImageDataBunch; Train: LabelList (5912 items) x: ImageList Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224) y: CategoryList pomeranian,saint_bernard,scottish_terrier,Persian,newfoundland Path: /root/.fastai/data/oxford-iiit-pet/images; Valid: LabelList (1478 items) x: ImageList Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224) y: CategoryList Birman,great_pyrenees,shiba_inu,Siamese,great_pyrenees Path: /root/.fastai/data/oxford-iiit-pet/images; Test: None . The above line tells fastai what kind of dataset we have, and how it is structured. There are various different classes for different kinds of deep learning dataset and problem. Here we&#39;re using ImageDataBunch. The first part of the class name will generally be the type of data you have, such as image, or text. The second part will generally be getting the file names. The third part refers to our pattern of Regex. . We define the Transforms that we need. A Transform contains code that is applied automatically during training; fastai includes many pre-defined Transforms. df_tfms is applied to a batch of items at a time using the GPU, so they&#39;re particularly fast. . Why 224 pixels? This is the standard size for historical reasons (old pretrained models require this size exactly), but we can pass pretty much anything. If we increase the size, we&#39;ll often get a model with better results (since it will be able to focus on more details) but at the price of speed and memory consumption; or vice versa if you decrease the size. . The data.normalize function converts the image into pixel values of color channels (RGB) which ranges from 0 - 255, converting it into matrix form. This is where our GPU is very good at i.e. Matrix Operations. . In&nbsp;[11]: data.show_batch(rows=3, figsize=(7,6)) . In&nbsp;[12]: print(data.classes) len(data.classes), data.c . [&#39;Abyssinian&#39;, &#39;Bengal&#39;, &#39;Birman&#39;, &#39;Bombay&#39;, &#39;British_Shorthair&#39;, &#39;Egyptian_Mau&#39;, &#39;Maine_Coon&#39;, &#39;Persian&#39;, &#39;Ragdoll&#39;, &#39;Russian_Blue&#39;, &#39;Siamese&#39;, &#39;Sphynx&#39;, &#39;american_bulldog&#39;, &#39;american_pit_bull_terrier&#39;, &#39;basset_hound&#39;, &#39;beagle&#39;, &#39;boxer&#39;, &#39;chihuahua&#39;, &#39;english_cocker_spaniel&#39;, &#39;english_setter&#39;, &#39;german_shorthaired&#39;, &#39;great_pyrenees&#39;, &#39;havanese&#39;, &#39;japanese_chin&#39;, &#39;keeshond&#39;, &#39;leonberger&#39;, &#39;miniature_pinscher&#39;, &#39;newfoundland&#39;, &#39;pomeranian&#39;, &#39;pug&#39;, &#39;saint_bernard&#39;, &#39;samoyed&#39;, &#39;scottish_terrier&#39;, &#39;shiba_inu&#39;, &#39;staffordshire_bull_terrier&#39;, &#39;wheaten_terrier&#39;, &#39;yorkshire_terrier&#39;] . Out[12]: (37, 37) . Here, data.classes and data.c are used to print the names of the classes in our dataset. . Training: resnet34&lt;/b&gt;&lt;/center&gt;&lt;/h3&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Now we will start training our model. We will use a convolutional neural network backbone and a fully connected head with a single hidden layer as a classifier. . We are going to use a method called Transfer Learning here. . Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks. We can use it for all kinds of visual recognition tasks. It takes 1/100th of the time and also 1/100th of the data to give us state-of-the-art results which we ll find out later. . Using a pretrained models is the most important method we have to allow us to train more accurate models, more quickly, with less data, and less time and money. . resnet34 as the name suggests has 34 fully connected neural network which has arready been trained on 1.3 million images on the ImageNet Dataset. We are just using the weights that that the model has already learned from it. . What is meant by weights in machine learning? Weights are used to connect the each neurons in one layer to the every neurons in the next layer. Weights near zero mean changing this input will not change the output. . We are building a model which will take images as input and will output the predicted probability for each of the categories (in this case, it will have 37 outputs). . We will train for 4 epochs (4 cycles through all our data). . What are epochs in Machine Learning? Each time the model sees the whole training data. . In&nbsp;[13]: learn = cnn_learner(data, models.resnet34, metrics=error_rate) . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth . . learn knows three things - 1. What&#39;s my data 2.What&#39;s my architecture 3.What metric is being used. . cnn_learner also has a parameter pretrained, which defaults to True (so it&#39;s used in this case), which sets the weights in your model to values that have already been trained by experts to recognize a thousand different categories across 1.3 million photos (using the famous ImageNet dataset). . We must divide our data into three sets. A training set, a test set and a Validation set. . Training set - The data used for fitting the model which is 80% of our data; it does not include any data from the validation set. . Validation set - 20% of our data which is set of data held out from training, used only for measuring how good the model is called our Validation set which helps us avoid overfitting. . Test set - A test set is a dataset that is independent of the training dataset, but that follows the same probability distribution as the training dataset. If a model fit to the training dataset also fits the test dataset well,minimal overfitting has taken place. A better fitting of the training dataset as opposed to the test dataset usually points to overfitting. . Overfitting - Training a model in such a way that it remembers specific features of the input data, rather than generalizing well to data not seen during training. Neural networks are really good at memorizing data and we got to avoid that. That is why validation set is always necessary. . Architecture - resnet34 is basically the Architecture for our model which is the template of the model that we&#39;re trying to fit; the actual mathematical function that we&#39;re passing the input data and parameters to. . In&nbsp;[14]: learn.model . Out[14]: Sequential( (0): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (4): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (5): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten() (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=1024, out_features=512, bias=True) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=37, bias=True) ) ) . In&nbsp;[15]: learn.fit_one_cycle(4) . epoch train_loss valid_loss error_rate time . 0 | 0.971628 | 0.345953 | 0.108931 | 01:15 | . 1 | 0.574955 | 0.294661 | 0.089310 | 01:15 | . 2 | 0.391528 | 0.236975 | 0.075101 | 01:18 | . 3 | 0.287441 | 0.220404 | 0.069012 | 01:16 | . Our model is around 93.10% Accurate. . fit_one_cycle is what starts our training process. For more details, please refer the article - one-cycle-policy. This explains all the mathematical details behind this concept brilliantly. . In&nbsp;[0]: learn.save(&#39;stage-1&#39;) . learn.save() saves the learned weights/co-efficients. We ll save our original model so that we can use it later to fine tune things if required. . Results&lt;/b&gt;&lt;/center&gt;&lt;/h3&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; We will first see which were the categories that the model most confused with one another. We will try to see if what the model predicted was reasonable or not. In this case the mistakes look reasonable. This is an indicator that our classifier is working correctly. . Furthermore, when we plot the confusion matrix, we can see that the distribution is heavily skewed: the model makes the same mistakes over and over again but it rarely confuses other categories. This suggests that it just finds it difficult to distinguish some specific categories between each other; this is normal behaviour. . In&nbsp;[17]: interp = ClassificationInterpretation.from_learner(learn) losses,idxs = interp.top_losses() len(data.valid_ds)==len(losses)==len(idxs) . Out[17]: True . Loss Function - It&#39;s a method of evaluating how well specific algorithm models the given data. . In&nbsp;[19]: interp.plot_top_losses(9, figsize=(15,11)) . plot_top_losses function - Lets see the documentation for fastai library. It explains each and every argument/results very well, which are also self-explanatory and easy to read. We ll use the doc() function for it. . In&nbsp;[20]: doc(interp.plot_top_losses) . _cl_int_plot_top_losses[source][test] . _cl_int_plot_top_losses(k, largest=True, figsize=(12, 12), heatmap:bool=False, heatmap_thresh:int=16, alpha:float=0.6, cmap:str=&#39;magma&#39;, show_text:bool=True, return_fig:bool=None) → Optional[Figure] . &times;No tests found for _cl_int_plot_top_losses. To contribute a test please refer to this guide and this discussion. . Show images in top_losses along with their prediction, actual, loss, and probability of actual class. . Show in docs . In&nbsp;[21]: interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . Whats a Confusion Matrix? . In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one. Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another). . It is a special kind of contingency table, with two dimensions (&quot;actual&quot; and &quot;predicted&quot;), and identical sets of &quot;classes&quot; in both dimensions (each combination of dimension and class is a variable in the contingency table). . Intuitively, we can say that the dark colored boxes represent the number of times the prediction by our model being actually correct. . In&nbsp;[22]: interp.most_confused(min_val=2) . Out[22]: [(&#39;Ragdoll&#39;, &#39;Birman&#39;, 12), (&#39;american_pit_bull_terrier&#39;, &#39;staffordshire_bull_terrier&#39;, 7), (&#39;boxer&#39;, &#39;american_bulldog&#39;, 6), (&#39;Russian_Blue&#39;, &#39;British_Shorthair&#39;, 4), (&#39;staffordshire_bull_terrier&#39;, &#39;american_bulldog&#39;, 4), (&#39;Bengal&#39;, &#39;Egyptian_Mau&#39;, 3), (&#39;Egyptian_Mau&#39;, &#39;Bengal&#39;, 3), (&#39;Siamese&#39;, &#39;Birman&#39;, 3), (&#39;american_pit_bull_terrier&#39;, &#39;american_bulldog&#39;, 3), (&#39;staffordshire_bull_terrier&#39;, &#39;american_pit_bull_terrier&#39;, 3), (&#39;Bengal&#39;, &#39;Abyssinian&#39;, 2), (&#39;Russian_Blue&#39;, &#39;Abyssinian&#39;, 2), (&#39;basset_hound&#39;, &#39;beagle&#39;, 2), (&#39;chihuahua&#39;, &#39;shiba_inu&#39;, 2), (&#39;yorkshire_terrier&#39;, &#39;havanese&#39;, 2)] . most_confused - fastai Library provides this great function for a better understanding of the results. The first argument represents &quot;The Actual Class or Label&quot;, the second one represents &quot;What our model Predicted&quot; and the third one says &quot;How many times it was predicted wrong by our model&quot; . Unfreezing, fine-tuning and Learning Rates&lt;/b&gt;&lt;/center&gt;&lt;/h3&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Since our model is working as we expect it to, we will unfreeze our model and train some more and lets see if we can make our model better. . In&nbsp;[0]: learn.unfreeze() . Unfreeze - It just says &quot;Please train the whole model&quot;. It trains all the layers at same speed. . In&nbsp;[24]: learn.fit_one_cycle(1) . epoch train_loss valid_loss error_rate time . 0 | 0.870884 | 0.424474 | 0.132612 | 01:17 | . As we can see that our error got worse. . In&nbsp;[0]: learn.load(&#39;stage-1&#39;); . Lets load our previously saved model. . In&nbsp;[26]: learn.lr_find() . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 0.00% [0/1 00:00&lt;00:00] epoch train_loss valid_loss error_rate time . &lt;progress value=&#39;58&#39; class=&#39;&#39; max=&#39;184&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 31.52% [58/184 00:21&lt;00:46 0.7265] &lt;/div&gt; &lt;/div&gt; LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Learning Rate - It basically means &quot;How quickly am I updating the weights in our model&quot;. . lr_find function helps us to find the best learning rate for our model. . In&nbsp;[27]: learn.recorder.plot() . We can see our losses are minimum between the learning rates 1e-06 and 1e-04. Rest of the layers&#39; learning rates are distributed between the two values. . In&nbsp;[28]: learn.unfreeze() learn.fit_one_cycle(4, max_lr=slice(1e-6,1e-4)) . epoch train_loss valid_loss error_rate time . 0 | 0.272802 | 0.222898 | 0.073072 | 01:16 | . 1 | 0.253456 | 0.206037 | 0.071719 | 01:17 | . 2 | 0.221768 | 0.201265 | 0.064953 | 01:17 | . 3 | 0.212356 | 0.198743 | 0.064953 | 01:17 | . We can see that our Model is around 93.51% Accurate!!! Let&#39;s try a bigger neural network and see if we can make it even better. . In&nbsp;[0]: . Training: Resnet50&lt;/b&gt;&lt;/center&gt;&lt;/h3&gt;Now we will train in the same way as before but with one specific condition: instead of using resnet34 as our architecture we will use resnet50 (resnet34 is a 34 layer residual network while resnet50 has 50 layers. This is a link to the Resnet paper. . resnet50 usually performs better because it is a deeper network with more parameters. Let&#39;s see if we can achieve a higher performance here. To help it along, let&#39;s us use larger images too, since that way the network can see more detail. We reduce the batch size a bit since otherwise this larger network will require more GPU memory. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; In&nbsp;[29]: data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=299, bs=bs//2) data.normalize(imagenet_stats) . Out[29]: ImageDataBunch; Train: LabelList (5912 items) x: ImageList Image (3, 299, 299),Image (3, 299, 299),Image (3, 299, 299),Image (3, 299, 299),Image (3, 299, 299) y: CategoryList pomeranian,saint_bernard,scottish_terrier,english_cocker_spaniel,Persian Path: /root/.fastai/data/oxford-iiit-pet/images; Valid: LabelList (1478 items) x: ImageList Image (3, 299, 299),Image (3, 299, 299),Image (3, 299, 299),Image (3, 299, 299),Image (3, 299, 299) y: CategoryList British_Shorthair,pug,havanese,miniature_pinscher,havanese Path: /root/.fastai/data/oxford-iiit-pet/images; Test: None . In&nbsp;[30]: learn = cnn_learner(data, models.resnet50, metrics=error_rate) . Downloading: &#34;https://download.pytorch.org/models/resnet50-19c8e357.pth&#34; to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth . . In&nbsp;[31]: learn.lr_find() learn.recorder.plot() . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 0.00% [0/1 00:00&lt;00:00] epoch train_loss valid_loss error_rate time . &lt;progress value=&#39;88&#39; class=&#39;&#39; max=&#39;369&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 23.85% [88/369 00:26&lt;01:23 12.4846] &lt;/div&gt; &lt;/div&gt; LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; In&nbsp;[32]: learn.fit_one_cycle(8) . epoch train_loss valid_loss error_rate time . 0 | 0.671309 | 0.341902 | 0.106901 | 01:55 | . 1 | 0.690459 | 0.270349 | 0.092693 | 01:54 | . 2 | 0.566171 | 0.367124 | 0.105548 | 01:54 | . 3 | 0.465545 | 0.250472 | 0.071719 | 01:55 | . 4 | 0.309690 | 0.243163 | 0.064953 | 01:55 | . 5 | 0.303274 | 0.216579 | 0.058187 | 01:55 | . 6 | 0.184373 | 0.176970 | 0.051421 | 01:53 | . 7 | 0.166427 | 0.175980 | 0.050744 | 01:54 | . In&nbsp;[0]: learn.save(&#39;stage-1-50&#39;) . We managed an accuracy around 95%. That&#39;s a pretty Accurate Model. Let&#39;s try fine tuning it and see if it gives us any better result. If we can&#39;t make it any better, we can always go back to our previous model. . In&nbsp;[34]: learn.unfreeze() learn.fit_one_cycle(3, max_lr=slice(1e-6,1e-4)) . epoch train_loss valid_loss error_rate time . 0 | 0.160864 | 0.175341 | 0.053451 | 01:59 | . 1 | 0.175153 | 0.184160 | 0.056157 | 01:56 | . 2 | 0.120222 | 0.181478 | 0.056834 | 01:57 | . We were not able to make our model better so let&#39;s load the previously saved model. . In&nbsp;[35]: learn.load(&#39;stage-1-50&#39;) . Out[35]: Learner(data=ImageDataBunch; Train: LabelList (5912 items) x: ImageList Image (3, 299, 299),Image (3, 299, 299),Image (3, 299, 299),Image (3, 299, 299),Image (3, 299, 299) y: CategoryList pomeranian,saint_bernard,scottish_terrier,english_cocker_spaniel,Persian Path: /root/.fastai/data/oxford-iiit-pet/images; Valid: LabelList (1478 items) x: ImageList Image (3, 299, 299),Image (3, 299, 299),Image (3, 299, 299),Image (3, 299, 299),Image (3, 299, 299) y: CategoryList British_Shorthair,pug,havanese,miniature_pinscher,havanese Path: /root/.fastai/data/oxford-iiit-pet/images; Test: None, model=Sequential( (0): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (5): Sequential( (0): Bottleneck( (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (6): Sequential( (0): Bottleneck( (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (4): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (5): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (7): Sequential( (0): Bottleneck( (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten() (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=4096, out_features=512, bias=True) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=37, bias=True) ) ), opt_func=functools.partial(&lt;class &#39;torch.optim.adam.Adam&#39;&gt;, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[&lt;function error_rate at 0x7fce7cfc8268&gt;], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath(&#39;/root/.fastai/data/oxford-iiit-pet/images&#39;), model_dir=&#39;models&#39;, callback_fns=[functools.partial(&lt;class &#39;fastai.basic_train.Recorder&#39;&gt;, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (10): ReLU(inplace=True) (11): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (13): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (17): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (19): ReLU(inplace=True) (20): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (21): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (22): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (23): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (24): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (26): ReLU(inplace=True) (27): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (28): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (29): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (30): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (31): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (32): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (33): ReLU(inplace=True) (34): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (36): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (37): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (38): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (40): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (42): ReLU(inplace=True) (43): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (44): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (45): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (46): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (47): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (48): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (49): ReLU(inplace=True) (50): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (51): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (52): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (53): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (54): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (55): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (56): ReLU(inplace=True) ), Sequential( (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (4): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (6): ReLU(inplace=True) (7): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False) (8): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (9): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (13): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (14): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (15): ReLU(inplace=True) (16): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (19): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (20): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (21): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (22): ReLU(inplace=True) (23): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (25): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (26): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (27): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (28): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (29): ReLU(inplace=True) (30): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (32): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (33): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (34): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (35): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (36): ReLU(inplace=True) (37): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (38): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (39): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (40): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (41): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (42): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (43): ReLU(inplace=True) (44): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (45): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (48): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (49): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (50): ReLU(inplace=True) (51): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False) (52): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (53): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (54): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (55): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (56): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (57): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (58): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (59): ReLU(inplace=True) (60): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (61): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (62): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (63): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (64): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (65): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (66): ReLU(inplace=True) ), Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): AdaptiveMaxPool2d(output_size=1) (2): Flatten() (3): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (4): Dropout(p=0.25, inplace=False) (5): Linear(in_features=4096, out_features=512, bias=True) (6): ReLU(inplace=True) (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): Dropout(p=0.5, inplace=False) (9): Linear(in_features=512, out_features=37, bias=True) )], add_time=True, silent=False) . In&nbsp;[36]: interp = ClassificationInterpretation.from_learner(learn) . Finally, lets interpret our results using the function most_confused() . In&nbsp;[37]: interp.most_confused(min_val=2) . Out[37]: [(&#39;american_pit_bull_terrier&#39;, &#39;staffordshire_bull_terrier&#39;, 10), (&#39;staffordshire_bull_terrier&#39;, &#39;american_pit_bull_terrier&#39;, 6), (&#39;Ragdoll&#39;, &#39;Birman&#39;, 5), (&#39;Birman&#39;, &#39;Ragdoll&#39;, 4), (&#39;american_bulldog&#39;, &#39;staffordshire_bull_terrier&#39;, 4), (&#39;Bengal&#39;, &#39;Egyptian_Mau&#39;, 3), (&#39;British_Shorthair&#39;, &#39;Russian_Blue&#39;, 3), (&#39;Egyptian_Mau&#39;, &#39;Bengal&#39;, 2), (&#39;Russian_Blue&#39;, &#39;British_Shorthair&#39;, 2), (&#39;Siamese&#39;, &#39;Birman&#39;, 2), (&#39;american_bulldog&#39;, &#39;american_pit_bull_terrier&#39;, 2), (&#39;american_bulldog&#39;, &#39;saint_bernard&#39;, 2), (&#39;beagle&#39;, &#39;basset_hound&#39;, 2), (&#39;chihuahua&#39;, &#39;miniature_pinscher&#39;, 2), (&#39;staffordshire_bull_terrier&#39;, &#39;american_bulldog&#39;, 2)] . Conclusion: We managed to make our model 95% Accurate using resnet50 Architecture. There are a number of architecture to choose from. We can even use resnet101 with as the name suggests containing 101 layers of fully connected layers. Hence, I conclude this project. . &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; . .",
            "url": "https://ankur2099.github.io/Blogger2099/2020/03/17/Deep-Learning-with-Pytorch_1.0-Pets-Classification-Project.html",
            "relUrl": "/2020/03/17/Deep-Learning-with-Pytorch_1.0-Pets-Classification-Project.html",
            "date": " • Mar 17, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "A Self-Taught Machine Learning and Deep Learning Enthusiast. An Avid Learner and a Cartoon Lover. I have worked on a number of projects and I thought it would be a good idea to blog about those. I have learned a lot of things just by reading other people’s blog and I thought to do the same. Please feel free to point out my mistakes so that I can learn from it and improve my skills. Thanks!❤ .",
          "url": "https://ankur2099.github.io/Blogger2099/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}